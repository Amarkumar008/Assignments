{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Houses are one of the necessary need of each and every person around the globe. It is a very large market and there are various companies working in the domain. Data science comes as a very important tool to solve problems in the domain to help the companies increase their overall revenue and profits.\n",
    "\n",
    "A US-based housing company named Surprise Housing has decided to enter the Australian market. The company is looking at prospective properties to buy houses to enter the market. The company uses data analytics to purchase houses at a price below their actual values and flip them at a higher price.\n",
    "\n",
    "The purpose is to build a model using Machine Learning in order to predict the actual value of the prospective properties and decide whether to invest in them or not. The requirement is to to model the price of houses with the available independent variables in the test data.\n",
    "\n",
    "The data is divided into two i.e. train and test data set. Model building will be done with the help of train data set and the same would be used on the test data set to predict house prices. The company has collected a data set from the sale of houses in Australia and is provided in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\HP-15\\housing_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 1168 records and 81 attributes in the dataset initially.\n",
    "\n",
    "The \"Id\" column in the above dataset does not have any relationship with the dependent/target variable as ID will have a unique identity for each record. Hence, we drop the column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping \"Id\" column from the dataset because it has no relation with target varaible\n",
    "df.drop(\"Id\",axis=1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if the Id column is dropped by visualizing gist of data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly noticable that the \"Id\" column has been removed from the above data set. Since the data set is huge, the whole data cannot be seen, hence maximum data needs to be visualized to understand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying maximum columns for better understaning of data\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see all the columns being displayed in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking size of present dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying maximum rows to visualize data type of all attributes\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset now consits of 1168 rows and 80 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking data types of the attributes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to check the null values present in the data to clean data before proceeding. We cannot analyse data without removing null values in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the null(NaN) values present in columns \"PoolQC\", \"Fence\", \"MiscFeature\" & \"Alley\" are more than 70% of the records in the data set. So the variables does not seem to be of any importance in analysing the sale price, hence removing them from the data set.\n",
    "\n",
    "The other null values present will be treated using SimpleImputer and Imputer techniques. Null values in data set needs to be treated in order to make an analysis on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping variables with maximum null values i.e. near to 70% of data are null values\n",
    "df.drop(\"PoolQC\",axis=1,inplace=True)\n",
    "df.drop(\"Fence\",axis=1,inplace=True)\n",
    "df.drop(\"MiscFeature\",axis=1,inplace=True)\n",
    "df.drop(\"Alley\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking number of variables left in the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noticable that 4 variables have been dropped from the data set.\n",
    "\n",
    "Now will treat null values in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling Nan values using 'most_frequent' of simle Imputer for categorical data\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp=SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "df['GarageQual']=imp.fit_transform(df['GarageQual'].values.reshape(-1,1))\n",
    "df['GarageCond']=imp.fit_transform(df['GarageCond'].values.reshape(-1,1))\n",
    "df['GarageFinish']=imp.fit_transform(df['GarageFinish'].values.reshape(-1,1))\n",
    "df['GarageType']=imp.fit_transform(df['GarageType'].values.reshape(-1,1))\n",
    "df['FireplaceQu']=imp.fit_transform(df['FireplaceQu'].values.reshape(-1,1))\n",
    "df['BsmtFinType2']=imp.fit_transform(df['BsmtFinType2'].values.reshape(-1,1))\n",
    "df['BsmtFinType1']=imp.fit_transform(df['BsmtFinType1'].values.reshape(-1,1))\n",
    "df['BsmtExposure']=imp.fit_transform(df['BsmtExposure'].values.reshape(-1,1))\n",
    "df['BsmtCond']=imp.fit_transform(df['BsmtCond'].values.reshape(-1,1))\n",
    "df['BsmtQual']=imp.fit_transform(df['BsmtQual'].values.reshape(-1,1))\n",
    "df['MasVnrType']=imp.fit_transform(df['MasVnrType'].values.reshape(-1,1))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used \"most_frequent\" strategy of Simple Imputer to fill null values of categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NaN values of categorical data is filled. Now we will deal with the Null values in numeric data. To deal with numeric data, we fill the values using median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries to treat Null values using median\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treating null values using Imputer\n",
    "for i in df.columns:\n",
    "    if df[i].isnull().sum()!=0:\n",
    "        df[i]=imp.fit_transform(df[i].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking null values again\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now observe that our null values have been treated. Let's visualize and check if we still have any Null Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing null values through heatmap\n",
    "import seaborn as sns\n",
    "sns.heatmap(df.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot see any Null Values in our dataset. Hence, we can proceed forward with visualizing our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making DataFrame for the Nominal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying nominal variables into a new dataframe\n",
    "df_nominal=df[['MSZoning','Street','LotShape', 'LandContour', 'Utilities', 'LotConfig','LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType','HouseStyle','RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType','ExterQual', 'ExterCond', 'Foundation', 'BsmtQual','BsmtCond', 'BsmtExposure', 'BsmtFinType1','BsmtFinType2', 'Heating','HeatingQC', 'CentralAir', 'Electrical','KitchenQual','Functional','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','SaleType','SaleCondition']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new separate dataframe to store all categorical data and make an analysis on it separetly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking columns of new nominal dataframe created\n",
    "df_nominal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cheking shape of new nominal dataframe\n",
    "df_nominal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1168 rows and 39 columns in the nominal DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the nominal/categorical data we will use countplot as it will gives frequency of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries for Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing different columns using countplot\n",
    "ncol,nrow=10,4\n",
    "ab=df_nominal.columns.values\n",
    "plt.figure(figsize=(20,40))\n",
    "for index,i in enumerate(ab):\n",
    "    ab=plt.subplot(ncol,nrow,index+1)\n",
    "    sns.countplot(df[i])\n",
    "    plt.title(f\"plot {i}\")\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the records in the data set consists of the \"All public Utilities (E,G,W,& S)\" in the 'Utilities' variable. Since the data in 'Utilities' column does not make any difference to the data set, we delete the column.\n",
    "\n",
    "In MSZoning (general zoning classification of the sale) variable, maximum values belong to the \"Residential Low Density\" category, then from \"Residential Medium Density\" and least from \"C(all)\" category.\n",
    "\n",
    "Type of road access to property ('Street') is \"Paved\" for most of the records.\n",
    "\n",
    "General shape of property ('LotShape') is \"Regular\" for most of the cases, second most is \"Slightly irregular\" and hardly some properties have \"Irregular\" property shape.\n",
    "\n",
    "Flatness of the property ('LandContour') is mostly \"Near Flat/Level\" for maximum records and least records have \"Depression\" flatness.\n",
    "\n",
    "Lot configuration ('LotConfig') belongs to \"Inside lot\" for most of the properties in the data set, second most belongs to \"Corner lot\" and least belongs to \"Frontage on 3 sides of property\" category.\n",
    "\n",
    "Maximum records have \"Gentle slope\" and very few have \"Severe Slope\" for variable Slope of property ('LandSlope').\n",
    "\n",
    "Proximity to various conditions ('Condition1') are \"Normal\" in most of the cases and least are \"Within 200' of East-West Railroad\".\n",
    "\n",
    "Proximity to various conditions ('Condition2') is 'Normal' for around 1500 records. Hardly the property has other various conditions.\n",
    "\n",
    "'BldgType' is \"1Fam\" i.e. \"Single-family Detached\" for nearly 1000 records.\n",
    "\n",
    "Style of dwelling ('HouseStyle') is \"One story\" which is maximum for nearly 580 records, \"Two story\" for nearly 360 records, \"One and one-half story\" for around 110 properties.\n",
    "\n",
    "Type of roof ('RoofStyle') is \"Gable\" for more than 900 values and around 200 values for \"Hip\".\n",
    "\n",
    "Roof material is \"Standard (Composite) Shingle\" for more than 110 properties in the dataset.\n",
    "\n",
    "The value for 'Masonry veneer type' is \"None\" for 700 records, \"Brick Face\" for nearly 350 records, \"Stone\" for nearly 100 records and least for \"Brick Common\".\n",
    "\n",
    "Quality of the material on the exterior ('ExterQual') is \"Average/Typical\" for more than 700 properties, \"Good\" for nearly 400 properties and least are \"Excellent\".\n",
    "\n",
    "Present condition of the material ('ExterCond') on the exterior is \"Average/Typical\" is for more than 1000 properties.\n",
    "\n",
    "Type of foundation ('Foundation') is maximum around 500 values for both \"Cinder Block\" & \"Poured Contrete\" and least for \"Wood\".\n",
    "\n",
    "Height of the basement ('BsmtQual') is \"Typical (80-89 inches)\" and \"Good (90-99 inches)\" for most of the records in data set.\n",
    "\n",
    "General condition of the basement ('BsmtCond') is \"Average/Typical\" for more tha 1000 properties and very few properties have \"Poor\" material.\n",
    "\n",
    "'BsmtExposure' (Refers to walkout or garden level walls) has \"No Exposure\" for nearly 800 properties.\n",
    "\n",
    "'BsmtFinType1' (Rating of basement finished area) is maximum for \"Unfinished\" i.e 350 values, second max for \"Good Living Quarters\" and least have \"Low Quality\".\n",
    "\n",
    "'BsmtFinType2' (Rating of basement finished area) is maximum for \"Unfinished\" i.e more than 1000 records.\n",
    "\n",
    "Type of heating ('Heating') has maximum values for \"GasA - Gas forced warm air furnace\" category which is more than 1100 records.\n",
    "\n",
    "Central air conditioning ('CentralAir') is available (\"Yes\") for around 1100 properties.\n",
    "\n",
    "'Electrical' is \"Standard Circuit Breakers & Romex\" for more than 1000 properties.\n",
    "\n",
    "Kitchen quality ('KitchenQual') is \"Typical/Average\" & \"Good\" for most records in the data set.\n",
    "\n",
    "Home functionality ('Functional') is \"Typical Functionality\" for more than 1000 properties.\n",
    "\n",
    "Fireplace quality ('FireplaceQu') is \"Good \" for more than 800 properties and \"Average\" for around 250 properties which includes the maximum records.\n",
    "\n",
    "'GarageType' is \"Attchd\" for maximum records and second maximum records holds for \"Detchd\".\n",
    "\n",
    "'GarageFinish' holds maximum records for \"Unfinished\", second max for \"Rough Finished\" and least for \"Finished\" which below 300 properties.\n",
    "\n",
    "'GarageQual' is \"Typical/Average\" for around 1100 properties and very less for the rest.\n",
    "\n",
    "'GarageCond' is also \"Typical/Average\" for around 1100 properties and very less for the rest.\n",
    "\n",
    "Paved driveway ('PavedDrive') is \"Paved\" for more than 1000 records.\n",
    "\n",
    "'SaleType' is \"Warranty Deed - Conventional\" for around 1000 properties.\n",
    "\n",
    "'SaleCondition' is \"Normal Sale\" for nearly 950 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping column having no importance data since all the values inside the column are same\n",
    "df.drop(\"Utilities\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing data which was not clear in the above analysis\n",
    "ab=sns.countplot(x='Neighborhood', data=df_nominal)\n",
    "print(df_nominal['Neighborhood'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Neighborhood' of the maximum properties are \"NAmes: North Ames\" and \"College Creek\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab=sns.countplot(x='Exterior1st', data=df_nominal)\n",
    "print(df_nominal['Exterior1st'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exterior covering on house i.e. 'Exterior1st' are maximum for \"Vinyl Siding\" (maximum records), \"Hard Board\", \"Metal Siding\" and \"Wood Siding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab=sns.countplot(x='Exterior2nd', data=df_nominal)\n",
    "print(df_nominal['Exterior2nd'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Exterior2nd' has maximum records for \"Vinyl Siding\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making DataFrame of the Continuous type of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying our continuous data into a new dataframe\n",
    "df_continuous=df[['MSSubClass','LotFrontage','LotArea','OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd','MasVnrArea','BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF','1stFlrSF', '2ndFlrSF', 'LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageYrBlt','GarageCars', 'GarageArea','WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch','ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold','SalePrice']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a new separate dataframe to store all numeric data and make an analysis on it separetly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying columns\n",
    "df_continuous.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cheking shape of new numeric dataframe\n",
    "df_continuous.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing different columns using distplot\n",
    "ncol,nrow=10,4\n",
    "ab=df_continuous.columns.values\n",
    "plt.figure(figsize=(20,40))\n",
    "for index,i in enumerate(ab):\n",
    "    ab=plt.subplot(ncol,nrow,index+1)\n",
    "    sns.distplot(df[i])\n",
    "    plt.title(f\"plot {i}\")\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "\n",
    "The type of dwelling involved in the sale ('MSSubClass') is \"20 : 1-STORY 1946 & NEWER ALL STYLES\" for most of the cases.\n",
    "\n",
    "Linear feet of street connected to property ('LotFrontage') is max between the range 50 & 100.\n",
    "\n",
    "Lot size in square feet ('LotArea') is has max records between values 0 & 25000.\n",
    "\n",
    "Rates the overall material and finish of the house ('OverallQual') has maximum records for values 5, 6 and 7.\n",
    "\n",
    "Rates the overall condition of the house ('OverallCond') has maximum record for value 5.\n",
    "\n",
    "Maximum records have Original construction date ('YearBuilt') as 2000.\n",
    "\n",
    "Maximum records for Remodel date (same as construction date if no remodeling or additions) ['YearRemodAdd'] has values between 2000 and 2010.\n",
    "\n",
    "The values of most of the properties for Masonry veneer area in square feet ('MasVnrArea') is 0 but the values range between 0 to 1750.\n",
    "\n",
    "The values of most of the properties for Type 1 finished square feet ('BsmtFinSF1') is 0 but the values range between 0 to 6000 having most of them ranging from 0 to 2000.\n",
    "\n",
    "The values of most of the properties for Type 2 finished square feet ('BsmtFinSF2') is 0 but the values range between 0 to 1500.\n",
    "\n",
    "The values for Unfinished square feet of basement area ('BsmtUnfSF') lies max between 0 to 1000.\n",
    "\n",
    "Total square feet of basement area ('TotalBsmtSF') has max records ranging between values 500 to 2000.\n",
    "\n",
    "First Floor square feet ('1stFlrSF') has maximum records ranging between values 500 to 1500.\n",
    "\n",
    "Second floor square feet ('2ndFlrSF') has maximum records at value 0 and also few between values 500 & 1500.\n",
    "\n",
    "Above grade (ground) living area square feet ('GrLivArea') has maximum records ranging between values 900 to 3000.\n",
    "\n",
    "Basement full bathrooms ('BsmtFullBath') has more records for value 0 than value 1.\n",
    "\n",
    "Variables such as 'LowQualFinSF', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea' & 'MiscVal' contains maximum records at value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using multivariate analysis to analyise continuous data\n",
    "sns.pairplot(df_continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, the data is huge and contains too many variables, the cummulative analysis of data is difficult to make. Though we can still observe some independent variables making a positive correlation with the target variable. We will try to understand this data using correlations.\n",
    "\n",
    "Converting Nominal data into numeric data representation is very important in order to understand data appropriately. Hence, we will use data encoding techniques to convert string values into floating numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing library for encoding and creating instance for the same \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc=OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting object datatype into float values\n",
    "for i in df.columns:\n",
    "    if df[i].dtypes=='object':\n",
    "        df[i]=enc.fit_transform(df[i].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying Conversion \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that our object datatype has been now converted into float values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describing present columns in the dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describing mean, median, min, max values of data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing data description\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(22,7))\n",
    "sns.heatmap(df.describe(),annot=True,linewidths=0.1,linecolor='blue',fmt='0.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "\n",
    "The standard deviation of few columns in the dataset are huge which means that the values in these columns are largely scattered and are not near to the mean values. They are very far away from their mean values. The standard devation of other columns is not too high which shows us a normal distribution of data and no skewness.\n",
    "\n",
    "We assume that since the min & max values in every column contains a range difference, it indicates a possibility of having few outliers and skewness in data.\n",
    "\n",
    "The only columns to have high values in the data set is the target variable 'SalePrice' and 'LotArea'.\n",
    "\n",
    "The data does not contain negative values in any of it's variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation of Columns with the Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting correlation of input features with the target Variable\n",
    "plt.figure(figsize=(22,7))\n",
    "sns.heatmap(df.corr(),annot=True,linewidths=0.1,linecolor='black',fmt='0.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Many positive correlations of independent variables can be seen with the 'SalePrice' target variable.\n",
    "\n",
    "Since there are lots of variables present in the data set we are not able to see correlations of variables clearly in the above diagram.\n",
    "\n",
    "Hence, we will sort the correlations of independent variables with the target variable in an ascending order to understand how highly or how low the variables are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting correlation in order with the Target Variable\n",
    "corr_matrix=df.corr()\n",
    "corr_matrix['SalePrice'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Correlation in order with the target variable to see which variables are more correlated\n",
    "plt.figure(figsize=(22,7))\n",
    "df.corr()['SalePrice'].sort_values(ascending=False).drop(['SalePrice']).plot(kind='bar',color='y')\n",
    "plt.xlabel('Feature',fontsize=14)\n",
    "plt.ylabel('column with target names',fontsize=14)\n",
    "plt.title('correlation',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23 variables out of 76 are negatively correlated to the target variable, rest all other are postively correlated.\n",
    "\n",
    "The most negatively correlated variables to the target variable are 'BsmtQual' & 'ExterQual'.\n",
    "\n",
    "There are many high positive correlations in the data.\n",
    "\n",
    "Since variables 'BsmtFinSF2', 'BsmtHalfBath' & 'MiscVal' are correlated to the target variable at -0.01% which hardly shows any correlation so we can delete these columns.\n",
    "\n",
    "The most positively correlated columns to the target variable are 'OverallQual', 'GrLivArea' and 'GarageCars'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking skewness to see if the data is normally distributed\n",
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping threshold +/-0.5 as the range for skewness, we can see high skewness in few variables such as 'LotArea', 'LowQualFinSF', '3SsnPorch', 'PoolArea' and 'MiscVal'. The skewness in these variables needs to be reduced in order to process data for analysis. Before dealing with the skewness, we'll first check outliers in data.\n",
    "\n",
    "We'll first visualizie skewness before checking outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing skewness on density graph\n",
    "#Example of multi variate analysis\n",
    "df.plot(kind='density',subplots=True,layout=(8,10),legend=False,sharex=False,figsize=(60,40))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum values are observed to be 0 in many numeric value variables from the above graph.\n",
    "\n",
    "Hence, we can observe high skewness in some of the variables.\n",
    "\n",
    "First will check outliers in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing outliers of different variables\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Visualizing outliers of different variables\n",
    "collist=df.columns.values\n",
    "ncol=6\n",
    "nrows=14\n",
    "plt.figure(figsize=(ncol,5*ncol))\n",
    "for i in range(1,len(collist)):\n",
    "    plt.subplot(nrows,ncol,i+1)\n",
    "    sns.boxplot(df[collist[i]],color='orange',orient='v')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see too many outliers present in the dataset. Tried removing outliers but it is giving an information loss of 58.73% which is huge data, hence data removal is not preffered. So instead of removing outliers, data transformation using power transfrom is applied on data to normalize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating the Column into x & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating x & y columns \n",
    "x=df.drop('SalePrice',axis=1)\n",
    "y=df['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the columns into x & y as input featues and target variable respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving Skewness using Power Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using power transform to remove skewness\n",
    "from sklearn.preprocessing import power_transform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "hf=power_transform(x,method='yeo-johnson')\n",
    "\n",
    "hf=pd.DataFrame(hf,columns=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying skewness\n",
    "hf.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness in some features are resolved but still can be seen in few variables even after applying transformation methods.\n",
    "\n",
    "In few variables, even though the skewness is high we cannot clear those variables from the data set because it has good correlation with the target variable and few variables have categorical data where we don't consider having skewness or outliers. \n",
    "\n",
    "Hence, we will proceed with scaling data using Standard Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing skewness after applying transformations on density graph\n",
    "hf.plot(kind='density',subplots=True,layout=(8,10),legend=False,sharex=False,figsize=(60,40))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many graphs have been normalized after applying transformations on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Data Using Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library for scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling our data to improve model performance\n",
    "sc=StandardScaler()\n",
    "x=sc.fit_transform(hf)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the data is Scaled.\n",
    "\n",
    "Since our Target Variable has continuous values, we use Linear Regression Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries to build linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Max accuracy at the best random state\n",
    "maxAccu=0\n",
    "maxRS=0\n",
    "for i in range(1,200):\n",
    "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.22,random_state=i)\n",
    "    lr=LinearRegression()\n",
    "    lr.fit(x_train,y_train)\n",
    "    pred=lr.predict(x_test)\n",
    "    acc=r2_score(y_test,pred)\n",
    "    if acc>maxAccu:\n",
    "        maxAccu=acc\n",
    "        maxRS=i\n",
    "print('Best accuracy is ',maxAccu,\" on Random_state\",maxRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting 85.90% accuracy at random state 140. Hence, we select random state as 140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into 80% training and 20% testing\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into Training and Testing giving 80% data for Training and 20% for Testing at best random state 140. We will use linear Regression to train our model as we have a continuous type of values in Target Variable 'SalePrice'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for Linear Regression\n",
    "lm=LinearRegression()\n",
    "lm.fit(x_train,y_train)\n",
    "pred=lm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying predicted and actual values\n",
    "print(\"Predicted Happiness Score: \",pred)\n",
    "print('actual Happiness Score: ',y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the Training accuracy and see how well we have trained our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training score\n",
    "lm.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are obtaining 82.41% training accuracy for our Model. Let's try finding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Errors\n",
    "print('error:')\n",
    "print('Mean absolute error:',mean_absolute_error(y_test,pred))\n",
    "print('Mean squared error:',mean_squared_error(y_test,pred))\n",
    "\n",
    "print('Root Mean squared error:',np.sqrt(mean_squared_error(y_test,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe errors in our model which we can try to minimise it using different regularization and ensemble techniques. Let's check the r2 score to check the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r2 score of the model\n",
    "print(r2_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are obtaining our r2 score as 87.03% which can also be due to overfitting or underfitting problems. Hence, we will use cross Validation to cross check if the model's performing accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation of The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding best cv score at a particular cv\n",
    "pred_train=lr.predict(x_train)\n",
    "pred_test=lr.predict(x_test)\n",
    "train_accuracy=r2_score(y_train,pred_train)\n",
    "test_accuracy=r2_score(y_test,pred_test)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "for j in range(2,10):\n",
    "    cv_score=cross_val_score(lr,x,y,cv=j)\n",
    "    cv_mean=cv_score.mean()\n",
    "    print(f\"At cross fold {j} the cv score is {cv_mean} and r2 score for training is {train_accuracy} and accuracy for the testing is {test_accuracy}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have handled the problem or overfitting and underfitting by checking the training and testing score at it's highest cv score. Let's try checking if we are covering all the points to attain accuracy by Visualizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for checking linear Regression data points with target variale\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x=y_test,y=pred_test,color='r')\n",
    "plt.plot(y_test,y_test,color='b')\n",
    "plt.xlabel('Actual charges',fontsize=14)\n",
    "plt.ylabel('Predicted charges',fontsize=14)\n",
    "plt.title('Linear Regression',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Best Fit line is covering many datapoints and can be seen near to the actual values which shows quite a good fit of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to use different testing techniques and compare them to achieve the best performance for the model and also deal with it's over fitting and under fitting problems and then save the best model with high performance. We are going to use hyper parameter tuning for doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries for hyper parameter tuning \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using ElasticNet Regression for our Model to solve overfitting and underfitting\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "parameters={'alpha':[.0001,.001,.01,.1,1,10],'random_state':list(range(0,10)),'selection':['cyclic','random']}\n",
    "en=ElasticNet()\n",
    "clf=GridSearchCV(en,parameters)\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for ElasticNet Regression\n",
    "en=ElasticNet(alpha=1,random_state=3,selection='random')\n",
    "en.fit(x_train,y_train)\n",
    "en.score(x_train,y_train)\n",
    "pred_en=en.predict(x_test)\n",
    "\n",
    "enn=r2_score(y_test,pred_en)\n",
    "enn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r2 score for ElasticNet Regression is 87.49%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Ridge Regression for our Model to solve overfitting and underfitting\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "parameters={'alpha':[.0001,.001,.01,.1,1,10],'random_state':list(range(0,10)),'solver':['auto','svd','cholesky']}\n",
    "rd=Ridge()\n",
    "clf=GridSearchCV(rd,parameters)\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for Ridge Regression\n",
    "rd=Ridge(alpha=10,random_state=0,solver='svd')\n",
    "rd.fit(x_train,y_train)\n",
    "rd.score(x_train,y_train)\n",
    "pred_rd=rd.predict(x_test)\n",
    "\n",
    "rdd=r2_score(y_test,pred_rd)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r2 score for Ridge Regression is 87.16%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Lasso Regression for our Model to solve overfitting and underfitting\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "parameters={'alpha':[.0001,.001,.01,.1,1,10],'random_state':list(range(0,10)),'selection':['cyclic','random']}\n",
    "ls=Lasso()\n",
    "clf=GridSearchCV(ls,parameters)\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for Lasso Regression\n",
    "ls=Ridge(alpha=10,random_state=1)\n",
    "ls.fit(x_train,y_train)\n",
    "ls.score(x_train,y_train)\n",
    "pred_ls=ls.predict(x_test)\n",
    "\n",
    "lss=r2_score(y_test,pred_ls)\n",
    "lss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r2 score for Lasso Regression is also 87.16%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, comparing all the three regularization techniques for regression i.e. ElasticNet, Ridge and Lasso, the model performs best for ElasticNet Regularization Regression technique at r2 score 87.49%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding best Parameters for Ada Boost Regressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "parameters={'random_state':list(range(0,10))}\n",
    "ab=AdaBoostRegressor()\n",
    "clf=GridSearchCV(ab,parameters)\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for Ada Boost Regressor\n",
    "ab=AdaBoostRegressor(random_state=4, n_estimators=100)\n",
    "ab.fit(x_train,y_train)\n",
    "ab.score(x_train,y_train)\n",
    "pred_y=ab.predict(x_test)\n",
    "\n",
    "abs=r2_score(y_test,pred_y)\n",
    "print('R2 score: ',abs*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the Cv score of the model\n",
    "abscore=cross_val_score(ab,x,y,cv=8)\n",
    "abc=abscore.mean()\n",
    "print(\"Cross Val Score: \",abc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r2 score for Ada Boost Regressor is 81.32% and that for it's cv score is 78.9% which does not give us much of a good performance model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we will try using Random Forest Regressor algorithm for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding best Parameters for RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "parameters={'random_state':list(range(0,10)),'criterion':['mse','mae']}\n",
    "rfg=RandomForestRegressor()\n",
    "clf=GridSearchCV(rfg,parameters)\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for RandomForestRegressor\n",
    "rfg=RandomForestRegressor(random_state=9, n_estimators=100, criterion='mse')\n",
    "rfg.fit(x_train,y_train)\n",
    "rfg.score(x_train,y_train)\n",
    "pred_y=rfg.predict(x_test)\n",
    "\n",
    "abs=r2_score(y_test,pred_y)\n",
    "print('R2 score: ',abs*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the Cv score of the model\n",
    "abscore1=cross_val_score(rfg,x,y,cv=8)\n",
    "abc1=abscore1.mean()\n",
    "print(\"Cross Val Score: \",abc1*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r2 score for RandomForestRegressor is 87.56% and it's cv score is 85.13% which has the best performance among all the other algorithms used for testing the performance of the model. The performance of ElasticNet Regularization technique is also good which is 87.49% where as for Random Forest Regressor is 87.56% which is a little better than the regularization technique. Hence, we use the RandomForestRegressor Ensemble technique for this model and save it as our best model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving The Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the best model\n",
    "import pickle\n",
    "filename='housing.pkl'\n",
    "pickle.dump(rfg,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "The test data consists of the similar variables as present in the train data set except the \"Target Variable\" but the number of records in the test data set is comparatively less than that of train data set. This data set is used to predict values of houses using the independent variables present in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Testing Dataset\n",
    "df1=pd.read_csv(r'C:\\Users\\HP-15\\housing_train.csv')\n",
    "#Visualizing Test dataset\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial variables in the test dataset (before data pre-processing)\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping \"Id\" column from the test dataset just like we did it in he trin data set\n",
    "df1.drop(\"Id\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking size of present dataset\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset now consits of 292 rows and 79 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking data types of all the attributes within the data set\n",
    "df1.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set consists of a combination of 'String', 'int' and 'float' data types.\n",
    "\n",
    "Since it is very important to check the null values present in the data to clean data before predicting values, hence we first check and resolve null values in data set using \"Simple Imputer\" techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Null values\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the null(NaN) values present in columns \"PoolQC\", \"Fence\", \"MiscFeature\" & \"Alley\" are more than 70% of the records in the data set. So the variables does not seem to be of any importance in analysing the sale price, hence removing them from the data set.\n",
    "\n",
    "The other null values present will be treated using SimpleImputer and Imputer techniques. Null values in data set needs to be treated in order to make an analysis on data.\n",
    "\n",
    "These are the same steps performed as we did while training the \"train\" data set. Hence, we have to follow the same procedure in order to predict values for \"test\" data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping variables with maximum null values i.e. near to 70% of data are null values\n",
    "df1.drop(\"PoolQC\",axis=1,inplace=True)\n",
    "df1.drop(\"Fence\",axis=1,inplace=True)\n",
    "df1.drop(\"MiscFeature\",axis=1,inplace=True)\n",
    "df1.drop(\"Alley\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking number of variables left in he dataset\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noticable that 4 variables have been dropped from the data set.\n",
    "\n",
    "Now will treat null values of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling Nan values using 'most_frequent' of simle Imputer for categorical data\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp=SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "df1['GarageQual']=imp.fit_transform(df1['GarageQual'].values.reshape(-1,1))\n",
    "df1['GarageCond']=imp.fit_transform(df1['GarageCond'].values.reshape(-1,1))\n",
    "df1['GarageFinish']=imp.fit_transform(df1['GarageFinish'].values.reshape(-1,1))\n",
    "df1['GarageType']=imp.fit_transform(df1['GarageType'].values.reshape(-1,1))\n",
    "df1['FireplaceQu']=imp.fit_transform(df1['FireplaceQu'].values.reshape(-1,1))\n",
    "df1['BsmtFinType2']=imp.fit_transform(df1['BsmtFinType2'].values.reshape(-1,1))\n",
    "df1['BsmtFinType1']=imp.fit_transform(df1['BsmtFinType1'].values.reshape(-1,1))\n",
    "df1['BsmtExposure']=imp.fit_transform(df1['BsmtExposure'].values.reshape(-1,1))\n",
    "df1['BsmtCond']=imp.fit_transform(df1['BsmtCond'].values.reshape(-1,1))\n",
    "df1['BsmtQual']=imp.fit_transform(df1['BsmtQual'].values.reshape(-1,1))\n",
    "df1['MasVnrType']=imp.fit_transform(df1['MasVnrType'].values.reshape(-1,1))\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used \"most_frequent\" strategy of Simple Imputer to fill null values of categorical data.\n",
    "\n",
    "Now again we'll check the null values present in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Null values\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries to treat Null values using median\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treating null values using Simple Imputer\n",
    "for i in df1.columns:\n",
    "    if df1[i].isnull().sum()!=0:\n",
    "        df1[i]=imp.fit_transform(df1[i].values.reshape(-1,1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking null values again\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now observe that our null values have been treated. Let's visualize and check if we still have any Null Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing null values through heatmap\n",
    "import seaborn as sns\n",
    "sns.heatmap(df1.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot see any Null Values in our dataset. Hence, we can proceed forward with visualizing our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making DataFrame for the Nominal Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying nominal variables into a new dataframe\n",
    "df_nominal1=df1[['MSZoning','Street','LotShape', 'LandContour', 'Utilities', 'LotConfig','LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType','HouseStyle','RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType','ExterQual', 'ExterCond', 'Foundation', 'BsmtQual','BsmtCond', 'BsmtExposure', 'BsmtFinType1','BsmtFinType2', 'Heating','HeatingQC', 'CentralAir', 'Electrical','KitchenQual','Functional','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','SaleType','SaleCondition']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new separate dataframe to store all categorical data and make an analysis on it separetly. Same procedures that we followed in Training our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking columns of new nominal dataframe created\n",
    "df_nominal1.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cheking shape of new nominal dataframe\n",
    "df_nominal1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 292 rows and 39 columns in the nominal DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the nominal/categorical data we will use countplot as it gives frequency of the columns. Since we have already imported the required libraries earlier, we'll not import them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing different columns using countplot\n",
    "ncol,nrow=10,4\n",
    "ab=df_nominal1.columns.values\n",
    "plt.figure(figsize=(20,40))\n",
    "for index,i in enumerate(ab):\n",
    "    ab=plt.subplot(ncol,nrow,index+1)\n",
    "    sns.countplot(df1[i])\n",
    "    plt.title(f\"plot {i}\")\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Utilities column separately to get exact figure of values for records\n",
    "ab=sns.countplot(x='Utilities', data=df_nominal1)\n",
    "print(df_nominal1['Utilities'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation\n",
    "Since we have deleted the 'Utilities' variable in the Training data set as it contained similar value (\"AllPub\") for all records but in the above graph we can see 291 records have \"AllPub\" as its value but only 1 record has a different value i.e. \"NoSeWa\". Hence, we conclude the column to be of not much importance and delete it for the test data set as well.\n",
    "\n",
    "'MSZoning' has maximum records for value \"RL\" and second highest is for \"RM\".\n",
    "\n",
    "The most records in \"Street\" belongs to value \"Pave\".\n",
    "\n",
    "The 'LotShape' for most properties is \"Reg\" and second most is \"IR1\".\n",
    "\n",
    "'LandContour' is maximum for \"Lvl\".\n",
    "\n",
    "The highest records in 'LotConfig' is for value \"Inside LotConfig\".\n",
    "\n",
    "'LandSlope' has maximum records for \"Gentle slope\".\n",
    "\n",
    "Variables such as \"Neighborhood\", \"Exterior1st\", \"Exterior2nd\" are unpredictable here as it consists of too many categories but have been separately explained for the train data set.\n",
    "\n",
    "'Condition1' and 'Condition2' is \"Normal\" for most of the properties.\n",
    "\n",
    "'BldgType' is \"1Fam-Single-family Detached\" for around 240 properties.\n",
    "\n",
    "'HouseStyle' has values \"1Story\" and \"2Story\" for 140+ and 80 records respectively.\n",
    "\n",
    "'RoofStyle' is \"Gable\" for more than 200 properties.\n",
    "\n",
    "'RoofMatl', 'BsmtCond', 'BsmtFinType2' and 'Heating' holds maximum records i.e. 250+ for values \"Standard (Composite) Shingle\", \"Typical - slight dampness allowed\", \"Unfinshed\" and \"GasA\" respectively.\n",
    "\n",
    "More than 250 properties have 'CentralAir'.\n",
    "\n",
    "Variables 'Electrical', 'Functional', 'FireplaceQu' have 250+ records for values \"Standard Circuit Breakers & Romex\", \"Typ\" and \"Gd\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping column having no importance data since all the values inside the column are same except 1 record, hence does not serve of much importance in both train and test data set.\n",
    "df1.drop(\"Utilities\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making DataFrame of the Continuous type of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying our continuous data into a new dataframe\n",
    "df_continuous1=df1[['MSSubClass','LotFrontage','LotArea','OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd','MasVnrArea','BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF','1stFlrSF', '2ndFlrSF', 'LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageYrBlt','GarageCars', 'GarageArea','WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch','ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying columns\n",
    "df_continuous1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cheking shape of new numeric dataframe\n",
    "df_continuous1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing different columns using distplot\n",
    "ncol,nrow=10,4\n",
    "ab=df_continuous1.columns.values\n",
    "plt.figure(figsize=(20,40))\n",
    "for index,i in enumerate(ab):\n",
    "    ab=plt.subplot(ncol,nrow,index+1)\n",
    "    sns.distplot(df1[i])\n",
    "    plt.title(f\"plot {i}\")\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "The type of dwelling involved in the sale ('MSSubClass') is \"20 : 1-STORY 1946 & NEWER ALL STYLES\" for most of the cases.\n",
    "\n",
    "Linear feet of street connected to property ('LotFrontage') is max between the range 50 & 75.\n",
    "\n",
    "Lot size in square feet ('LotArea') has max records between values 0 & 25000.\n",
    "\n",
    "Rates the overall material and finish of the house ('OverallQual') has maximum records for values 5, 6 and 7.\n",
    "\n",
    "Rates the overall condition of the house ('OverallCond') has maximum record for value 5.\n",
    "\n",
    "Maximum records have Original construction date ('YearBuilt') as 2000.\n",
    "\n",
    "Maximum records for Remodel date (same as construction date if no remodeling or additions) ['YearRemodAdd'] has values between 2000 and 2010.\n",
    "\n",
    "The values of most of the properties for Masonry veneer area in square feet ('MasVnrArea') is 0 but the values range between 0 to 1200.\n",
    "\n",
    "The values of most of the properties for Type 1 finished square feet ('BsmtFinSF1') is 0 but the values range between 0 to 2000 having most of them ranging from 0 to 1000.\n",
    "\n",
    "The values of most of the properties for Type 2 finished square feet ('BsmtFinSF2') is 0 but the values range between 0 to 1200.\n",
    "\n",
    "The values for Unfinished square feet of basement area ('BsmtUnfSF') lies max between 0 to 1000.\n",
    "\n",
    "Total square feet of basement area ('TotalBsmtSF') has max records ranging between values 500 to 2000.\n",
    "\n",
    "First Floor square feet ('1stFlrSF') has maximum records ranging between values 500 to 1500.\n",
    "\n",
    "Second floor square feet ('2ndFlrSF') has maximum records at value 0 and also few between values 500 & 1000.\n",
    "\n",
    "Above grade (ground) living area square feet ('GrLivArea') has maximum records ranging between values 900 to 3000.\n",
    "\n",
    "Basement full bathrooms ('BsmtFullBath') has more records for value 0 than value 1.\n",
    "\n",
    "Variables such as 'LowQualFinSF', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea' & 'MiscVal' contains maximum records at value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using multivariate analysis to analyise continuous type of data\n",
    "sns.pairplot(df_continuous1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, the data is huge and contains too many variables, the cummulative analysis of data is difficult to make. Though we can still observe some independent variables making a positive correlation with the target variable. We will try to understand this data using correlations.\n",
    "\n",
    "Converting Nominal data into numeric data representation is very important in order to understand data appropriately. Hence, we will use data encoding techniques to convert string values into floating numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing library for encoding and creating instance for the same \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc=OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting object datatype into float values\n",
    "for i in df1.columns:\n",
    "    if df1[i].dtypes=='object':\n",
    "        df1[i]=enc.fit_transform(df1[i].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying Conversion \n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that our object datatype has been now converted into float values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describing present columns in the dataset\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describing present columns in the dataset\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 292 rows and 74 columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describing mean, median, min, max values of data\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing data description\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(22,7))\n",
    "sns.heatmap(df1.describe(),annot=True,linewidths=0.1,linecolor='blue',fmt='0.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "The standard deviation of few columns in the dataset are huge which means that the values in these columns are largely scattered and are not near to the mean values. They are very far away from their mean values. The standard devation of other columns is not too high which shows us a normal distribution of data and no skewness.\n",
    "\n",
    "We assume that since the min & max values in every column contains a range difference, it indicates a possibility of having few outliers and skewness in data.\n",
    "\n",
    "The only column to have high values in the test data set is the independent variable 'LotArea'.\n",
    "\n",
    "The data does not contain negative values in any of it's variables.\n",
    "\n",
    "Correlation of Columns with the Target Variable\n",
    "Since we don't have the target variable \"SalePrice\" in the test data set, we cannot find the correlation of independent variables with the dependent/ target variable. We have already found the correlation between the independent variables and target vaiables from the train data set.\n",
    "\n",
    "The target variable is not available in the test data set because we are going to predict the values of \"SalePrice\" of test data set with the algorithm trained using the train data set. Hence, no correlations can be found for this data set.\n",
    "\n",
    "The test data set is going to be processed just as the train data set since the data information and pattern is similar so same pre-processing steps will also be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking skewness to see if the data is normally distributed\n",
    "df1.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping threshold +/-0.5 as the range for skewness, we can see high skewness in few variables such as 'LotArea', 'LowQualFinSF', 'Condition2', '3SsnPorch', 'RoofMatl', 'Street' and 'MiscVal'. The skewness in these variables needs to be reduced in order to process data for analysis. The skewed data needs to be normalized. Before dealing with the skewness, we'll first check outliers in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing outliers of different variables\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Visualizing outliers of different variables\n",
    "collist=df1.columns.values\n",
    "ncol=9\n",
    "nrows=9\n",
    "plt.figure(figsize=(ncol,5*ncol))\n",
    "for i in range(1,len(collist)):\n",
    "    plt.subplot(nrows,ncol,i+1)\n",
    "    sns.boxplot(df1[collist[i]],color='orange',orient='v')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see too many outliers present in the dataset. Tried removing outliers but it is giving an information loss of 58.73% which is huge data, hence data removal is not preffered. So instead of removing outliers, data is tranferred using power transfrom and is applied on data to normalize it.\n",
    "\n",
    "There's no need of separating x and y since we already don't have y variable existing in the test data set. So, we skip the step.\n",
    "\n",
    "\n",
    "Resolving Skewness using Power Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using power transform to remove skewness\n",
    "from sklearn.preprocessing import power_transform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "hf1=power_transform(df1,method='yeo-johnson')\n",
    "\n",
    "hf1=pd.DataFrame(hf1,columns=df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying skewness\n",
    "hf1.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness in some features can still be seen even after applying transformation methods.\n",
    "\n",
    "In few variables, even though the skewness is high we cannot clear those variables from the data set because it had good correlation with the target variable in the train data set and few variables have categorical data where we don't consider having outliers or outliers.\n",
    "\n",
    "Also we cannot drop the columns in the test data set if not deleted in the test data set, other wise we cannot predict the \"Sales Price\" without having similar variables in both data sets.\n",
    "\n",
    "Hence, we will proceed with scaling data using Standard Scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Data Using Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling our data to improve model performance\n",
    "sc=StandardScaler()\n",
    "x1=sc.fit_transform(hf1)\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Best Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading model\n",
    "import pickle\n",
    "fitted_model=pickle.load(open('housing.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting The Test File using the Pre Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Prediction of \"SalesPrice\" on test data using algorithm trained on Train data\n",
    "predictions=fitted_model.predict(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction of \"SalePrice\" values on test data \n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 score of the predicted model\n",
    "fitted_model=pickle.load(open('housing.pkl','rb'))\n",
    "result=fitted_model.score(x_test,y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we are able to predict the test data at 87.56% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
